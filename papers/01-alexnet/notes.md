# AlexNet

> 2012 年发表，首次在大规模图像分类任务中成功应用深度卷积神经网络，标志着深度学习在计算机视觉领域的崛起。

## 网络架构

AlexNet 由 **8 层**组成：5 个卷积层 + 3 个全连接层，最后接 softmax 输出 1000 类（对应 ImageNet 的类别数）。

![AlexNet 架构图](./解释_assets/media/image2.png)

网络分为上下两路，分别运行在两块 GPU 上：

- **第 1、2 层**：两块 GPU 各自独立工作，互不通信
- **第 3 层**：两块 GPU 开始交互，交换特征信息
- **第 4、5 层**：继续各自卷积
- **全连接层**：两路各输出 2048 维，拼接为 4096 维，最终通过线性层映射到 1000 类

**卷积的直觉理解**：通过逐层卷积，特征图的空间尺寸不断缩小（一个小像素对应原图的一大块区域），而通道数不断增加——可以理解为每个通道负责识别物体的一种模式（如猫的爪子、猫的眼睛）。

## 核心创新

### 1. ReLU 激活函数

用 ReLU 替代传统的 Sigmoid / Tanh，显著加速训练收敛。

![ReLU vs Tanh 训练误差对比](./解释_assets/media/image1.png)

- **实线**：ReLU，约 5 个 epoch 即收敛到 0.25
- **虚线**：Tanh，35 个 epoch 才达到相近水平

### 2. 双 GPU 并行训练

将模型拆分到两块 GPU 上并行计算，突破了当时单卡显存的限制。这一设计在今天已不再必要（现代 GPU 显存足够），但其**模型并行**的思想对后续大模型训练仍有启发。

### 3. Dropout 正则化

在全连接层中使用 Dropout（训练时随机丢弃部分神经元），有效缓解过拟合。

- **原始解释**：每次训练得到的是原始模型的一个子模型，最终效果等价于多模型融合
- **现代理解**：后来学者证明，Dropout 在线性模型上等价于 L2 正则化，因此现在普遍将其视为一种正则化手段

此外，AlexNet 还使用了**局部响应归一化（LRN）**和**重叠池化**来提升性能。

## 训练细节

### 优化器：带动量的 SGD

$$v_{i+1} := 0.9 \cdot v_i - 0.0005 \cdot \epsilon \cdot w_i - \epsilon \cdot \left\langle \frac{\partial L}{\partial w} \bigg|_{w_i} \right\rangle_{D_i}$$

$$w_{i+1} := w_i + v_{i+1}$$

其中 $\epsilon$ 为学习率，$v$ 为动量变量（momentum = 0.9）。动量的作用是让参数更新不被当前梯度过度左右，有助于跳出局部最优。

### 学习率策略

| 策略 | 做法 | 使用场景 |
|------|------|----------|
| **AlexNet 原文** | 初始 lr = 0.01，每当验证误差不再下降时手动缩小 10 倍 | 早期常用 |
| **现代主流（Cosine Annealing with Warmup）** | 先从极小值线性增长到目标 lr（Warmup），再按余弦函数逐步衰减 | Transformer、ResNet 等大模型训练 |

## 实验成果

在 ILSVRC-2012（ImageNet 大规模视觉识别挑战赛）中，AlexNet 取得 **top-5 错误率 15.3%**，比第二名低约 **10 个百分点**，是绝对的碾压级表现。

## 历史意义

AlexNet 的成功直接推动了后续一系列经典 CNN 架构的诞生：VGG、GoogLeNet、ResNet 等，奠定了现代计算机视觉的基础。
